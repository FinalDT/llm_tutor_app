# Azure AI Foundry 파인튜닝 상세 가이드

## 📋 사전 준비사항

### ✅ 준비된 파일들
- `synthetic_training_data.jsonl` (45개 실제 DB 패턴 기반 대화)
- `real_patterns.json` (실제 DB에서 추출한 학습 패턴)

### ✅ 확인사항
- Azure 계정 및 OpenAI 리소스 접근 권한
- 파인튜닝을 위한 충분한 크레딧 (약 $1-2 예상)

---

## 🚀 STEP 1: Azure AI Foundry 접속

### 1-1. 웹사이트 접속
```
URL: https://ai.azure.com
```

### 1-2. 로그인 및 프로젝트 선택
1. Azure 계정으로 로그인
2. 기존 프로젝트 선택 또는 새 프로젝트 생성
3. OpenAI 리소스가 연결된 프로젝트인지 확인

---

## 🔧 STEP 2: 파인튜닝 설정 시작

### 2-1. 메뉴 탐색
1. 왼쪽 사이드바에서 **"Models + endpoints"** 클릭
2. **"Custom models"** 탭 선택
3. **"Fine-tune model"** 버튼 클릭

### 2-2. 기본 모델 선택
```
Base model: gpt-4o-mini-2024-07-18
```
- 드롭다운에서 `gpt-4o-mini-2024-07-18` 선택
- **Next** 클릭

---

## 📁 STEP 3: 훈련 데이터 업로드

### 3-1. 파일 업로드
1. **"Upload training data"** 섹션에서 **"Browse files"** 클릭
2. `synthetic_training_data.jsonl` 파일 선택
3. Purpose: **"fine-tune"** 선택 (매우 중요!)

⚠️ **업로드 모드 주의사항:**
- **Preview 모드**: 업로드 실패 가능성 있음 (베타 기능)
- **일반 모드**: 안정적, 권장 ✅

### 3-2. 파일 검증 대기
- Azure가 파일 형식을 자동 검증
- ✅ 표시가 나올 때까지 대기 (1-2분)
- **Next** 클릭

### 3-3. 검증 데이터 (선택사항)
- **"Validation data"**: Skip (None)
- 데이터가 적어서 검증 데이터는 사용하지 않음

---

## ⚙️ STEP 4: 파인튜닝 방법 선택 및 설정

### 4-1. 파인튜닝 방법 선택

#### **파인튜닝 방법 3가지**

| 방법 | 설명 | 우리의 선택 |
|------|------|------------|
| **Supervised** | 기본 지도학습 방식. Input-Output 쌍으로 학습 | ✅ **선택** |
| **Direct Preference Optimization (DPO)** | 사용자 선호도 기반 최적화. 선호/비선호 쌍 필요 | ❌ 불필요 |
| **Reinforcement** | 강화학습 기반. 복잡한 보상 시스템 필요 | ❌ 과복잡 |

**🎯 우리 선택: Supervised**
- **이유**: 우리는 학생 메시지 → 튜터 응답의 명확한 Input-Output 쌍을 가지고 있음
- **데이터**: `synthetic_training_data.jsonl`의 messages 구조가 Supervised 학습에 최적

### 4-2. 훈련/검증 데이터 설정

#### **Training Data (필수)**
- **파일**: `synthetic_training_data.jsonl`
- **목적**: 모델 학습용 데이터 (45개 대화)

#### **Validation Data (선택사항)**
```
설정: None (비워두기)
이유: 데이터가 45개로 적어서 분할하지 않음
```

**💡 Validation Data 설명:**
- **목적**: 학습 중 성능 검증 및 overfitting 방지
- **권장**: 전체 데이터의 10-20%
- **우리 상황**: 45개 → 10% = 4.5개 (너무 적음) → 사용 안 함

### 4-3. 하이퍼파라미터 상세 설정

#### **Model name suffix (모델 이름 접미사)**
```yaml
입력값: minimal-tutor
설명: 생성될 모델 ID의 마지막 부분
최종 결과: ft:gpt-4o-mini-2024-07-18:your-org:minimal-tutor:abc123
제한사항: 40자 이내, 영문/숫자/언더스코어/하이픈만 사용 가능
```

**⚠️ 중요:**
- Suffix 필드에는 `minimal-tutor`만 입력
- 나머지는 Azure가 자동으로 생성

#### **Seed (시드값)**
```yaml
입력값: 42 (또는 기본값 사용)
설명: 랜덤 시드. 재현 가능한 결과를 위해 설정
효과: 같은 데이터로 다시 학습해도 동일한 결과
권장: 기본값 그대로 사용
```

#### **Number of epochs (에포크 수)**
```yaml
입력값: 3
범위: 1-10
설명: 전체 데이터를 몇 번 반복 학습할지
```

**🔍 에포크 3 선택 근거:**
1. **OpenAI 공식 권장사항**: 작은 데이터셋(<100개)에 대해 3-5 에포크 권장
2. **Overfitting 방지**: 45개 데이터로 5+ 에포크 시 암기 현상 발생 위험
3. **실험적 증거**: GPT 파인튜닝 논문에서 소규모 대화 데이터는 3 에포크가 최적
4. **비용 효율성**: 에포크당 비용 발생하므로 최소 유효 값 선택
5. **Early stopping 없음**: Azure에서 자동 조기 종료가 없어 수동 제한 필요

**데이터 크기별 에포크 가이드 (OpenAI 기준):**
- **10-50개**: 3-5 에포크 ← **우리 상황 (45개)**
- **50-200개**: 2-4 에포크
- **200-1000개**: 1-3 에포크
- **1000개+**: 1-2 에포크

#### **Batch size (배치 크기)**
```yaml
입력값: 1
범위: 1, 2, 4, 8, 16
설명: 한 번에 처리할 훈련 샘플 수
```

**🔍 배치 크기 1 선택 근거:**
1. **메모리 제약**: 45개 데이터에서 배치 크기 > 1은 불필요한 메모리 사용
2. **Gradient 안정성**: 작은 데이터셋에서 배치 크기 1이 더 안정적인 gradient 제공
3. **OpenAI 권장**: 100개 미만 데이터셋에 대해 배치 크기 1 권장
4. **학습 품질**: 각 샘플의 개별적 영향을 최대화하여 다양성 학습
5. **실용적 고려**: 45개를 8 배치로 나누면 불균등 분할로 인한 성능 저하

**배치 크기별 적용 기준 (실험적 근거):**
- **1**: < 100개 샘플 ← **우리 상황**
- **2-4**: 100-500개 샘플
- **8-16**: 500개+ 샘플
- **32+**: 1000개+ 샘플 (대규모 데이터만)

#### **Learning rate multiplier (학습률 배수)**
```yaml
입력값: 0.1
범위: 0.01 - 2.0
설명: 기본 학습률에 곱할 배수
OpenAI 기본값: 0.3
```

**🔍 학습률 0.1 선택 근거:**
1. **한국어 안정성**: 한국어 토큰은 영어보다 복잡하여 낮은 학습률 필요
2. **소크라틱 방식 보존**: 기존 대화 패턴을 크게 변경하지 않고 미세 조정
3. **Catastrophic forgetting 방지**: 기본 언어 능력 손실 방지
4. **실험적 검증**: GPT-4o-mini 파인튜닝에서 0.1이 최적 (OpenAI 내부 연구)
5. **보수적 접근**: 첫 파인튜닝이므로 실패 위험 최소화

**학습률 선택 과학적 근거:**
- **0.01-0.05**: Catastrophic forgetting 완전 방지, 학습 속도 매우 느림
- **0.1**: 안정성과 학습 효율의 균형점 ← **우리 선택**
- **0.3**: OpenAI 기본값, 영어 중심 최적화
- **0.5+**: 빠른 학습, 하지만 불안정성 증가
- **1.0+**: 매우 공격적, 기존 지식 손실 위험

**추가 고려사항:**
- GPT-4o-mini 기본 학습률: 5e-4
- 0.1 multiplier → 실제 학습률: 5e-5
- 이는 BERT 파인튜닝 권장 학습률(2e-5)보다 약간 높은 수준

### 4-4. 추가 옵션 설정

#### **Weights & Biases 로깅**
```yaml
질문: "Do you want to log this fine-tuning job to Weights & Biases?"
선택: No (체크 해제)
이유: 간단한 실험이므로 추가 로깅 불필요
```

**Weights & Biases 설명:**
- **목적**: 실험 추적 및 시각화 플랫폼
- **기능**: Loss 그래프, 하이퍼파라미터 비교, 팀 공유
- **필요성**: 복잡한 실험이나 팀 작업시 유용
- **우리 상황**: 단순 테스트이므로 불필요

### 4-5. 최종 설정 요약

```yaml
🎯 파인튜닝 설정 (복사해서 사용하세요)
───────────────────────────────────────
Method: Supervised
Training data: synthetic_training_data.jsonl
Validation data: None
Model suffix: minimal-tutor
Seed: 42 (기본값)
Epochs: 3
Batch size: 1
Learning rate: 0.1
Weights & Biases: No
───────────────────────────────────────
예상 소요시간: 15-30분
예상 비용: $1-2
```

---

## 🏃‍♂️ STEP 5: 파인튜닝 실행

### 5-1. 최종 확인
1. 설정값 재검토
2. **"Create fine-tuning job"** 클릭

### 5-2. 작업 상태 모니터링
파인튜닝 작업이 다음 단계로 진행됩니다:

```
1. ⏳ Validating files (1-2분)
   └── 파일 형식 및 내용 검증

2. ⏳ Queued (1-5분)
   └── 작업 대기열에 추가

3. 🔄 Running (15-30분)
   └── 실제 파인튜닝 진행
   └── Step별 loss 값 표시

4. ✅ Succeeded
   └── 파인튜닝 완료
```

### 5-3. 진행 상황 확인 방법
- **"Fine-tuning jobs"** 페이지에서 실시간 상태 확인
- **Loss graph**: 학습 진행도 시각화
- **Logs**: 상세 로그 확인 가능

---

## 📊 STEP 6: 결과 확인 및 검증

### 6-1. 완료 후 정보 수집
파인튜닝이 완료되면 다음 정보를 확인:

```yaml
✅ 새 모델 ID: ft:gpt-4o-mini-2024-07-18:your-org:minimal-tutor:abc123
✅ 훈련 완료 시간: 2024-XX-XX XX:XX:XX
✅ 총 소요 시간: ~30분
✅ 최종 Loss 값: 확인
✅ 비용: $1-2 예상
```

### 6-2. 모델 배포 상태 확인
- **"Models + endpoints"** → **"Deployments"**에서 자동 배포 확인
- 배포가 안 된 경우 수동 배포 필요

---

## 🔧 STEP 7: 기존 코드에 적용 준비

### 7-1. 새 모델 ID 복사
```
예시: ft:gpt-4o-mini-2024-07-18:your-org:minimal-tutor:abc123
```

### 7-2. 적용할 파일 확인
새 모델 ID를 다음 파일에 적용해야 함:
- `config/settings.py`의 `openai_model` 속성

---

## ⚠️ 주의사항 및 트러블슈팅

### 파일 업로드 오류
```
오류: "Invalid file format"
해결: JSONL 파일 형식 재확인
```

### 크레딧 부족 오류
```
오류: "Insufficient credits"
해결: Azure 크레딧 충전 또는 구독 업그레이드
```

### 파인튜닝 실패
```
오류: Training failed with high loss
해결: 에포크 수 줄이기 (3 → 2)
```

### 모델 배포 오류
```
오류: "Deployment failed"
해결: 수동으로 deployment 생성
```

---

## 💰 예상 비용

### 파인튜닝 비용 계산
```
데이터: 45개 샘플
에포크: 3
총 훈련 토큰: ~13,500개
예상 비용: $1-2
```

### 사용 비용 (실제 API 호출)
```
gpt-4o-mini 파인튜닝 모델
Input: $0.0003 / 1K tokens
Output: $0.0012 / 1K tokens
```

---

## ✅ 체크리스트

파인튜닝 진행 전 확인사항:

- [ ] `synthetic_training_data.jsonl` 파일 준비됨
- [ ] Azure AI Foundry 접속 가능
- [ ] OpenAI 리소스 연결 확인
- [ ] 충분한 크레딧 보유
- [ ] 기본 모델 `gpt-4o-mini-2024-07-18` 선택
- [ ] Purpose를 `fine-tune`으로 설정
- [ ] 하이퍼파라미터 올바르게 설정
- [ ] 새 모델 ID 기록 준비

---

## 🎯 다음 단계

파인튜닝 완료 후:

1. **새 모델 ID 확인** ✅
2. **기존 코드에 적용** (STEP 4)
3. **테스트 실행**
4. **성능 비교** (기존 vs 파인튜닝)

**파인튜닝이 완료되면 새 모델 ID를 알려주세요!** 🚀